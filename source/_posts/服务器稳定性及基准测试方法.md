---
title: 服务器稳定性及基准测试方法
comments: true
categories: [知识库]
date: 2025-09-11 15:40:24
keywords:
  - 服务器
  - 基准测试
  - 服务器压测
  - Stress-NG
  - Sysbench
  - 内存
  - CPU
  - 硬盘
tags:
  - 服务器
  - 基准测试
  - 服务器压测
  - Stress-NG
  - Sysbench
  - 内存
  - CPU
  - 硬盘
---

## 测试项

### CPU基准和稳定性测试

|测试项|测试工具|参考业务场景|测试方式|
|---|---|---|---|
|双精度浮点运算 float64|HPL|该项测试为HPC行业标准，TOP500超级计算机排行榜均采用该方式评估|测试3次取均值|
|稳定性|Stress-NG|CPU、内存、IO全面压力测试|正常压测|

### 内存基准和稳定性测试

<!-- more -->

|测试项|测试工具|选择理由|测试方式|
|---|---|---|---|
|内存带宽|STREAM Benchmark|HPC内存带宽的行业标准测试，TOP500超级计算机排行榜均采用该方式评估|Copy/Scale/Add/Triad测试3次取均值|
|稳定性测试|[Memtest86+](https://www.memtest.org/download/v7.20/mt86plus_7.20_64.grub.iso.zip)|持续运行数小时,测试稳定性|正常压测|

### 网络基准测试

|测试项|测试工具|参考业务场景|测试方式|
|---|---|---|---|
|带宽极限|iperf3|测试带宽极限性能是否接近理论极限值|测试3次取均值|
|吞吐测试|netperf TCP_RR / UDP_RR|微服务 RPC、数据库访问、API 调用、Kafka 消息传输|测试3次取均值|

### 硬盘测试

|测试项|测试工具|参考业务场景|测试方式|
|---|---|---|---|
|4k单队列|fio bs=4k iodepth=1|MySQL场景|测试3次取均值|
|4k 32队列|fio bs=4k iodepth=32|MongoDB场景|测试3次取均值|
|32k 32队列|fio bs=32k iodepth=32|高并发Web服务，Kafka日志刷盘，多线程缓存写入|测试3次取均值|
|1m 单队列|fio bs=1m iodepth=1|顺序读写，备份、视频流、镜像分发|测试3次取均值|
|1m 32队列|fio bs=1m iodepth=32|并发大文件读写，大规模备份，分布式存储，对象存储|测试3次取均值|

其中针对业务场景，根据单台服务器部署的业务数量适当设置`numjobs`以模拟操作系统/应用层并发。

例如单台服务器4个数据库，则`fio bs=4k iodepth=32 numjobs=4`。

`iodepth`对应单个线程的队列深度，用于存储设备并发能力测试

`numjobs`对应多个线程并发，用于测试操作系统/应用层并发能力

### 综合基准测试

|测试项|测试工具|参考业务场景|测试方式|
|---|---|---|---|
|综合性能指数|UnixBench|横向比较整机性能|测试3次取均值|

## 测试过程

### AMD 9F14平台测试

#### 安装CentOS 7.9.2009

替换源为清华大学CentOS-Vault源

```shell
sed -e "s|^mirrorlist=|#mirrorlist=|g" \
    -e "s|^#baseurl=http://mirror.centos.org/centos/\$releasever|baseurl=https://mirrors.tuna.tsinghua.edu.cn/centos-vault/7.9.2009|g" \
    -e "s|^#baseurl=http://mirror.centos.org/\$contentdir/\$releasever|baseurl=https://mirrors.tuna.tsinghua.edu.cn/centos-vault/7.9.2009|g" \
    -i.bak \
    /etc/yum.repos.d/CentOS-*.repo
```

该命令执行后会将原本的所有源配置文件备份为带`.bak`后缀名的备份

```shell
# 启用bash扩展通配符
shopt -s extglob

# 使用通配符快速删除不带.bak的文件
rm !(*.bak) -f

# 使用for循环快速去掉.bak后缀
for f in *.bak;do mv -- "$f" "${f%.bak}";done
```

`yum makecache`更新源

`yum install vim`安装vim编辑器

```shell
# 更新源
yum makecache
# 安装vim
yum install vim
# 编辑ssh配置文件
vim /etc/ssh/sshd_config
# 设置心跳
# 取消注释，设置为每60秒发送一次心跳重复9999999次
ClientAliveInterval 60
ClientAliveCountMax 99999
# 设置允许root用户登录
# 取消注释并配置为yes
PermitRootLogin yes
# 重启ssh服务
systemctl restart sshd
```

#### 准备CPU的Linpack测试

```shell
# 基础环境
yum install gcc gcc-c++ gcc-gfortran cmake python3 zlib* -y
# 安装OpenBLAS
yum install -y epel-release
yum install -y openblas openblas-devel
# 安装OpenMPI
yum install openmpi openmpi-devel
# 找到MPI的位置
find /usr -name "mpicc" 2>/dev/null
# 我的输出如下
/usr/lib64/openmpi/bin/mpicc
# 添加MPI到环境变量
echo 'export PATH=/usr/lib64/openmpi/bin:$PATH' >> ~/.bashrc
echo 'export LD_LIBRARY_PATH=/usr/lib64/openmpi/lib:$LD_LIBRARY_PATH' >> ~/.bashrc
source ~/.bashrc
# 临时生效
export PATH=/usr/lib64/openmpi/bin:$PATH
export LD_LIBRARY_PATH=/usr/lib64/openmpi/lib:$LD_LIBRARY_PATH

# 下载High Performance Linpack
# 去官网检查最新版https://www.netlib.org/benchmark/hpl/
wget https://www.netlib.org/benchmark/hpl/hpl-2.3.tar.gz
tar xvf hpl-2.3.tar.gz
mv hpl-2.3 hpl
mv hpl ~/
cd ~/hpl

# 从模板创建副本并改动编译器参数
# INTEL使用Make.Linux_Intel64
# AMD使用Make.Linux_ATHLON_FBLAS
cp setup/Make.Linux_ATHLON_FBLAS ./Make.AMD_OpenBLAS
vim Make.AMD_OpenBLAS

# 64行的ARCH自定义名称，需要和文件名后缀一致，如Make.AMD_OpenBLAS
# 后续make编译时会用到，如make arch=AMD_OpenBLAS
ARCH = AMD_OpenBLAS

# 配置openblas通用开源方案，保持测试结果的中立
# 避免MKL和AOCL BLIS/LibFLAME对于INTEL AMD的专项优化
# 修改配置使HPL走CBLAS接口，而不是老的Fortran BLAS
# 在配置文件中找到
HPL_DEFS     = $(F2CDEFS) $(HPL_OPTS) $(HPL_INCLUDES)
# 在这一行的上方加入以下内容
BLASLIB = -lopenblas
# 在末尾加上 -DHPL_CALL_CBLAS
HPL_DEFS     = $(F2CDEFS) $(HPL_OPTS) $(HPL_INCLUDES) -DHPL_CALL_CBLAS
# 找到CCFLAGS，添加-march=native
CCFLAGS      = $(HPL_DEFS) -fomit-frame-pointer -O3 -funroll-loops -W -Wall -march=native

# 默认的配置如下
LAdir        = $(HOME)/netlib/ARCHIVES/Linux_ATHLON
LAinc        =
LAlib        = $(LAdir)/libf77blas.a $(LAdir)/libatlas.a
# 修改为
LAdir   = /usr/lib64/openblas  # 或者你的 OpenBLAS 安装路径
LAinc   =
LAlib   = -lopenblas

# # 指定OpenMPI
# MPdir   = /usr
# MPinc   = -I/usr/include/openmpi-x86_64
# MPlib   = -L/usr/lib64 -lmpi

# 改为直接使用mpicc编译
# 修改CC
CC      = mpicc
# 在HPL_LIBS = $(HPLlib) $(LAlib) $(MPlib)的上方加入
MPlib = -lmpi
MPIinc = -I/usr/lib64/openmpi/include
MPilib = -L/usr/lib64/openmpi/lib -lmpi

# 复查一下系统中有没有openblas
ldconfig -p | grep openblas
# 输出通常如下所示
libopenblas64_.so.0 (libc6,x86-64) => /lib64/libopenblas64_.so.0
libopenblas64.so.0 (libc6,x86-64) => /lib64/libopenblas64.so.0
libopenblasp64_.so.0 (libc6,x86-64) => /lib64/libopenblasp64_.so.0
libopenblasp64.so.0 (libc6,x86-64) => /lib64/libopenblasp64.so.0
libopenblasp.so.0 (libc6,x86-64) => /lib64/libopenblasp.so.0
libopenblaso64_.so.0 (libc6,x86-64) => /lib64/libopenblaso64_.so.0
libopenblaso64.so.0 (libc6,x86-64) => /lib64/libopenblaso64.so.0
libopenblaso.so.0 (libc6,x86-64) => /lib64/libopenblaso.so.0
libopenblas.so.0 (libc6,x86-64) => /lib64/libopenblas.so.0

# 将g77指向gfortran
ln -s /usr/bin/gfortran /usr/bin/g77
# 编译HPL
make -f Make.top build_src arch=AMD_OpenBLAS
make -f Make.top build_tst arch=AMD_OpenBLAS


# 编辑HPL.dat
cd bin/AMD_OpenBLAS
mv HPL.dat HPL.dat.bak
vim HPL.dat

# HPL.dat范例，参数及其解析详见下方表格
HPLinpack benchmark input file
Innovative Computing Laboratory, University of Tennessee
HPL.out      output file name (if any)
8            device out (6=stdout,7=stderr,file)
1            # of problems sizes (N)
165120         Ns
1            # of NBs
384           NBs
0            PMAP process mapping (0=Row-,1=Column-major)
1            # of process grids (P x Q)
8            Ps
12            Qs
16.0         threshold
1            # of panel fact
2            PFACTs (0=left, 1=Crout, 2=Right)
1            # of recursive stopping criterium
4            NBMINs (>= 1)
1            # of panels in recursion
2            NDIVs
1            # of recursive panel fact.
1            RFACTs (0=left, 1=Crout, 2=Right)
1            # of broadcast
1            BCASTs (0=1rg,1=1rM,2=2rg,3=2rM,4=Lng,5=LnM)
1            # of lookahead depth
1            DEPTHs (>=0)
2            SWAP (0=bin-exch,1=long,2=mix)
64           swapping threshold
0            L1 in (0=transposed,1=no-transposed) form
0            U  in (0=transposed,1=no-transposed) form
1            Equilibration (0=no,1=yes)
8            memory alignment in double (> 0)
##### This line (no. 32) is ignored (it serves as a separator). ######
0                               Number of additional problem sizes for PTRANS
1200 10000 30000                values of N
0                               number of additional blocking sizes for PTRANS
40 9 8 13 13 20 16 32 64        values of NB

# 禁用防火墙
systemctl stop firewalld
setenforce 0
# 开始运行
mpirun --allow-run-as-root -np 96 --map-by core --bind-to core ./xhpl

# 使用htop查看硬件负载
yum install -y htop
htop

# 测试结果范例
================================================================================
HPLinpack 2.3  --  High-Performance Linpack benchmark  --   December 2, 2018
Written by A. Petitet and R. Clint Whaley,  Innovative Computing Laboratory, UTK
Modified by Piotr Luszczek, Innovative Computing Laboratory, UTK
Modified by Julien Langou, University of Colorado Denver
================================================================================

An explanation of the input/output parameters follows:
T/V    : Wall time / encoded variant.
N      : The order of the coefficient matrix A.
NB     : The partitioning blocking factor.
P      : The number of process rows.
Q      : The number of process columns.
Time   : Time in seconds to solve the linear system.
Gflops : Rate of execution for solving the linear system.

The following parameter values will be used:

N      :  165120
NB     :     384
PMAP   : Row-major process mapping
P      :       8
Q      :      12
PFACT  :   Right
NBMIN  :       4
NDIV   :       2
RFACT  :   Crout
BCAST  :  1ringM
DEPTH  :       1
SWAP   : Mix (threshold = 64)
L1     : transposed form
U      : transposed form
EQUIL  : yes
ALIGN  : 8 double precision words

--------------------------------------------------------------------------------

- The matrix A is randomly generated for each test.
- The following scaled residual check will be computed:
      ||Ax-b||_oo / ( eps * ( || x ||_oo * || A ||_oo + || b ||_oo ) * N )
- The relative machine precision (eps) is taken to be               1.110223e-16
- Computational tests pass if scaled residuals are less than                16.0

================================================================================
T/V                N    NB     P     Q               Time                 Gflops
--------------------------------------------------------------------------------
WR11C2R4      165120   384     8    12            1755.82             1.7094e+03
HPL_pdgesv() start time Mon Sep 15 12:53:01 2025

HPL_pdgesv() end time   Mon Sep 15 13:22:17 2025

--------------------------------------------------------------------------------
||Ax-b||_oo/(eps*(||A||_oo*||x||_oo+||b||_oo)*N)=   4.65407372e-04 ...... PASSED
================================================================================

Finished      1 tests with the following results:
              1 tests completed and passed residual checks,
              0 tests completed and failed residual checks,
              0 tests skipped because of illegal input values.
--------------------------------------------------------------------------------

End of Tests.
================================================================================
```

##### HPL.dat的参数及其说明

在线生成HPL.dat文件的网站[HPL.dat](https://www.advancedclustering.com/act_kb/tune-hpl-dat-file/)

|参数|说明|
|---|---|
|Nodes|对应CPU数量|
|Cores per Node|每个CPU多少核|
|Memory per Node(MB)|每个CPU有多少MB内存|
|Block Size(NB)|HPL运算的块大小|

NB 块大小计算方式：

**设置适当的块大小，使数据块能够很好的放入CPU的高速缓存（L2/L3 Cache），如果数据块不能放入高速缓存，则不得不从慢得多的主内存RAM中进行读取，从而导致性能瓶颈。**

NB最好是PQ的整数倍，以我的AMD 9A14为例，L3为384MB，96核心

HPL将矩阵N分块成NB*NB的小块，然后分布在一个二维进程网格P行Q列上

所以如果任何一方过长，就会造成通信路径变长，从而导致性能开销大

为了使性能达到最优，需要使这个二维网格接近正方形，也就是PQ值相近，且同时P*Q需要等于核心数，以确保每个核心都有数据块在计算

###### PQ计算公式

```shell
# 根据内核数量取近似的能开方的整数
# 例如96核取100开方
# 然后用核数除平方根递归直到整除
96/10=9.6
96/9=10.66
96/8=12
# PQ则为8*12
# 该网格便由8行12列组成，接近正方形，避免了过长的通信路径
# 每个进程需要将分解结果广播给其他进程，所以过长的通信路径就意味着过长的通信开销
# 如图
- - - - - - -
- - - · - - -
- - - - - - -
# 对比过长的通信路径
- - - - - - - - ·- - - - - - -
```

PQ数值为`8*12`，接下来计算NB，NB最好是PQ的整数倍，且需要能放入L3 Cache

Double精度每个元素8字节，则L3 384MB按照PQ计算为384/96=4MB，每个进程4MB

如果NB太小，会导致每次发送过于碎片化，增大通信开销，太大则可能导致Cache容量不足，通常取一半值

###### NB块大小计算公式

```shell
# 首先查询CPU的L3缓存
# 然后用L3缓存除以核心数，算出每个核心能使用的缓存均值
# 根据float类型占4字节，double类型占8字节计算
# 该性能测试为双精度浮点运算，取8字节
# 先计算每个核心能使用多少字节的缓存
4*1024*1024=4,194,304
# 然后除以双精度的8字节，计算出每个核心处理多少个双精度浮点数，也就是NB²
4194304/8=524288
# 然后开方得出NB
524288开方约为724
# 为了避免撑爆L3缓存，只取一半值
724/2=362
# 对其double的8字节64比特位
362/64=5.65625
# 取整数6
64*6=384
# 得出NB最优值为384
```

最终计算出PQ为`8*12`，NB为`384`

![20250915145114](https://img.hackerbs.com//20250915145114.png)

在线生成后的HPL.dat如下，HPL只会读取第一列，数字后方的是注释，带#的也是注释

```shell
HPLinpack benchmark input file
Innovative Computing Laboratory, University of Tennessee
HPL.out      output file name (if any)
6            device out (6=stdout,7=stderr,file)
1            # of problems sizes (N)
165120         Ns
1            # of NBs
384           NBs
0            PMAP process mapping (0=Row-,1=Column-major)
1            # of process grids (P x Q)
8            Ps
12            Qs
16.0         threshold
1            # of panel fact
2            PFACTs (0=left, 1=Crout, 2=Right)
1            # of recursive stopping criterium
4            NBMINs (>= 1)
1            # of panels in recursion
2            NDIVs
1            # of recursive panel fact.
1            RFACTs (0=left, 1=Crout, 2=Right)
1            # of broadcast
1            BCASTs (0=1rg,1=1rM,2=2rg,3=2rM,4=Lng,5=LnM)
1            # of lookahead depth
1            DEPTHs (>=0)
2            SWAP (0=bin-exch,1=long,2=mix)
64           swapping threshold
0            L1 in (0=transposed,1=no-transposed) form
0            U  in (0=transposed,1=no-transposed) form
1            Equilibration (0=no,1=yes)
8            memory alignment in double (> 0)
##### This line (no. 32) is ignored (it serves as a separator). ######
0                               Number of additional problem sizes for PTRANS
1200 10000 30000                values of N
0                               number of additional blocking sizes for PTRANS
40 9 8 13 13 20 16 32 64        values of NB
```

根据这份文件，运行命令如下：

```shell
mpirun --allow-run-as-root -np 96 --map-by core --bind-to core ./xhpl
# --allow-run-as-root 允许root用户运行
# -np 96 启动96个MPI进程
# --map-by core MPI进程核CPU逻辑核心进行绑定
# --bind-to core 将每个进程锁定到core上运行，避免cache miss
```

关于参数的解释在官网有详细说明[HPL Tuning](https://www.netlib.org/benchmark/hpl/tuning.html)

#### 进行内存的基准测试

使用[STREAM Benchmark](https://www.cs.virginia.edu/stream/)进行测试

[源代码](https://www.cs.virginia.edu/stream/FTP/Code/stream.c)

```shell
# 安装编译工具
yum install gcc -y

# 下载源码，或者复制，都行
wget https://www.cs.virginia.edu/stream/FTP/Code/stream.c

# 编译
gcc -O3 -fopenmp -DSTREAM_ARRAY_SIZE=80000000 stream.c -o stream

# 运行
./stream

# 直接输出到文件
./stream >> mem_test_0.log
```

#### 进行内存的稳定性测试

根据步骤中提供的下载链接，下载后的文件`mt86plus_7.20_64.grub.iso.zip`，版本以你最新的为准，上方表格中的链接如果旧了就在这里打开下载

[Memtest86+](https://www.memtest.org/)

![20250915173052](https://img.hackerbs.com//20250915173052.png)

建议使用这个版本，用有ventoy的盘直接引导就能用了，很方便，在带外iKVM直接挂载引导也行

启动界面